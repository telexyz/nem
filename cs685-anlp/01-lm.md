https://people.cs.umass.edu/~miyyer/cs685/slides/01-lm.pdf

https://www.youtube.com/watch?v=q6KvtdJzXlQ

![](files/01-00.jpg)

LLM rất mạnh, có thể dùng prompting để làm các tasks khác nhau!

![](files/01-01.jpg)

![](files/01-02.jpg)

Chain Rule quan trọng, nhất là khi áp dụng n-gram. Còn với NNLM thì dùng luôn prefix (w_1 w_2 ... w_i-1)

## N-gram

![](files/01-03.jpg)

![](files/01-04.jpg)

![](files/01-05.jpg)

![](files/01-06.jpg)

![](files/01-07.jpg)

## Perplexity
![](files/01-08.jpg)

> Mô hình ngôn ngữ tốt nhất là mô hình dự đoán tốt nhất các dữ liệu chưa có trong tập huấn luyện! (unseen data)
![](files/01-09.jpg)

![](files/01-10.jpg)

Trong thực tế chúng ta dùng log scale để tránh tràn số.
- p(w) biết được từ softmax đầu ra,
- tính __negative avg của log p(w) là ra được perplexity (pp)__
- pp càng thấp nghĩa là model càng tốt

![](files/01-11.jpg)

